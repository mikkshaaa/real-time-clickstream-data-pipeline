# ğŸš€ Real-Time Clickstream Data Pipeline (End-to-End Streaming Project)

ğŸš€ End-to-end real-time clickstream analytics pipeline using Kafka, Spark (Delta Lake), Airflow, DuckDB, and Power BI. Cloud-ready design with Bronze-Silver-Gold architecture, easily portable to Azure (ADLS, Databricks, Synapse).

---

## ğŸ—ï¸ Architecture

**Flow:**  
`Clickstream Simulator â†’ Kafka â†’ Spark Structured Streaming â†’ Delta Lake (bronze/silver/gold) â†’ Airflow (batch orchestration) â†’ DuckDB (Synapse simulation) â†’ Power BI`

![Flow](images/Flow.png)

---

## âš¡ Tech Stack

- **Storage:** Local filesystem (simulating Azure Data Lake Gen2 with `bronze/silver/gold` zones)
- **Streaming:** Apache Kafka (Docker)
- **Processing:** Apache Spark (PySpark + Delta Lake)
- **Orchestration:** Apache Airflow (optional, via pip or Docker)
- **Warehouse:** DuckDB (simulating Azure Synapse)
- **Visualization:** Power BI Desktop (free) or Metabase (open-source)

---

## ğŸ“‚ Project Structure

```
clickstream-pipeline/
â”œâ”€ docker/
â”‚  â””â”€ docker-compose-kafka.yml      # Kafka + Zookeeper setup
â”œâ”€ airflow/
â”‚  â””â”€ dags/
â”‚     â””â”€ etl_dag.py                 # Airflow DAG (optional)
â”œâ”€ src/
â”‚  â”œâ”€ data_generator.py             # Kafka producer (simulated clickstream)
â”‚  â”œâ”€ spark_streaming.py            # Streaming ingestion â†’ Delta (bronze)
â”‚  â”œâ”€ batch_transform.py            # Batch ETL (bronze â†’ silver â†’ gold)
â”‚  â””â”€ export_to_duckdb.py           # Export gold data to DuckDB/CSV
â”œâ”€ data_lake/                       # Local "ADLS" (bronze/silver/gold zones)
â””â”€ README.md
```

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone Repo & Setup Environment
```bash
git clone https://github.com/yourusername/clickstream-pipeline.git
cd clickstream-pipeline
python -m venv .venv
source .venv/bin/activate       # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install kafka-python pyspark delta-spark duckdb pandas
```

âš ï¸ Prerequisites:
- Install **Docker & Docker Compose**
- Install **Java 11 JDK** (required by Spark)
- (Optional) Install **Airflow** for orchestration

---

### 2. Start Kafka via Docker
```bash
cd docker
docker-compose -f docker-compose-kafka.yml up -d
```

Create topic:
```bash
docker exec -it $(docker ps --filter "ancestor=bitnami/kafka" -q) \
  /opt/bitnami/kafka/bin/kafka-topics.sh \
  --create --topic clickstream-events \
  --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
```

---

### 3. Run Clickstream Data Generator
```bash
python src/data_generator.py
```
â¡ï¸ Continuously sends synthetic user clickstream events to Kafka.

---

### 4. Start Spark Structured Streaming Job
```bash
python src/spark_streaming.py
```
â¡ï¸ Consumes Kafka events â†’ stores raw data in **Delta Lake (bronze)** under `./data_lake/bronze`.

---

### 5. Run Batch ETL Job (Bronze â†’ Silver â†’ Gold)
```bash
python src/batch_transform.py
```
â¡ï¸ Cleans and aggregates events, creating **session summary tables** in gold zone.

---

### 6. Export Gold Data to DuckDB / CSV
```bash
python src/export_to_duckdb.py
```
â¡ï¸ Loads curated data into DuckDB and exports sample CSV (`session_sample.csv`).

---

### 7. Visualization
- **Power BI Desktop (Windows):**
  - Open `session_sample.csv`
  - Build dashboards: DAU, page views, session duration, funnel conversion
- **Alternative (cross-platform):**
  - Run Metabase via Docker and connect to DuckDB/CSV

---

## ğŸ“Š Example Dashboards

### Daily Active Users
![DAU](images/dau.png)

### Average Session Duration per User
![Session Duration](images/session_duration.png)

### Distribution of Events per Session
![Events Distribution](images/events_distribution.png)

---

## ğŸš€ Run End-to-End (Quick Recap)

```bash
# Terminal 1 - start Kafka
docker-compose -f docker/docker-compose-kafka.yml up -d

# Terminal 2 - run Spark streaming (keep running)
python src/spark_streaming.py

# Terminal 3 - run clickstream generator
python src/data_generator.py

# Terminal 4 - batch ETL + export
python src/batch_transform.py
python src/export_to_duckdb.py
```

Then open CSV/Parquet in Power BI or Metabase to build dashboards ğŸ‰

---

## ğŸŒ Next Steps (Cloud Migration)

- Replace local `data_lake/` â†’ **Azure Data Lake Gen2 (ADLS Gen2)**
- Replace DuckDB â†’ **Azure Synapse Analytics**
- Deploy Kafka & Airflow â†’ **Azure Kubernetes Service (AKS)**
- Run Spark jobs â†’ **Azure Databricks**

---
